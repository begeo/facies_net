{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stub file to get things working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\envs\\facies_net\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Check we are using the right environment\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HOME_PATH = Path.home()\n",
    "DATA_PATH = Path.home().joinpath('data')\n",
    "CLASSES_PATH = Path.home().joinpath('code').joinpath('seispredcode').joinpath('facies_net').joinpath('class_addresses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial package imports per facies_net file\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Check on keras backend (using tensorflow)\n",
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports from facies_net code\n",
    "from facies_net_func.masterf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [str(DATA_PATH.joinpath('f3').joinpath('F3_entire.segy'))]\n",
    "cube_incr = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_filenames = [\n",
    " 'multi_else_ilxl.pts'\n",
    ",'multi_grizzly_ilxl.pts'\n",
    ",'multi_high_amp_continuous_ilxl.pts'\n",
    ",'multi_high_amplitude_ilxl.pts'\n",
    ",'multi_high_amplitude_utm.pts'\n",
    ",'multi_low_amp_dips_ilxl.pts'\n",
    ",'multi_low_amplitude_ilxl.pts'\n",
    ",'multi_low_coherency_ilxl.pts'\n",
    ",'multi_salt_ilxl.pts'\n",
    ",'multi_steep_dips_ilxl.pts'\n",
    ",'Other_ilxl.pts'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files = [str(CLASSES_PATH.joinpath(f)) for f in class_filenames]\n",
    "# Weighting of loss function to classes; must contain all classes in input data  \n",
    "class_weights = None\n",
    "# Should be the same as uniform allocation of weights, e.g. class_weights = {i:1.0 for i in np.arange(len(class_files))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    'files'         : class_files,\n",
    "    'epochs'        : 10,       # number of epochs we run on each training ensemble/mini-batch\n",
    "    'num_train_ex'  : 80000,    # number of training examples in each training epoch\n",
    "    'batch_size'    : 32,       # number of training examples fed to the optimizer as a batch\n",
    "    'val_split'     : 0.3,      # fraction of examples used for validation\n",
    "    'opt_patience'  : 10,       # number of epochs with the same accuracy before force breaking the training ensemble/mini-batch\n",
    "    'class_weights' : class_weights,     # optional weight dict to weight loss function\n",
    "    'data_augmentation' : ['None'],    # whether or not we are using data augmentation\n",
    "    'save_model'    : True,         # whether or not we are saving the trained model\n",
    "    'save_location' : 'F3\\\\BE_TEST_MODEL'    # file name for the saved trained model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary holding all the prediction parameters\n",
    "pred_dict = {\n",
    "    'keras_model'   : None, #keras.models.load_model('F3/10_epochs_80000_examples.h5'), # input model to be used for prediction, to load a model use: keras.models.load_model('write_location')\n",
    "    'section_edge'  : np.asarray([150, 700, 350, 1200, 150, 1700]), # inline and xline section to be predicted (all depths), must contain xline\n",
    "    'show_feature'  : False,     # Show the distinct features before they are combined to a prediction\n",
    "    'xline'         : 775,      # xline used for classification (index)(should be within section range)\n",
    "    'num_class'     : len(train_dict['files']),     # number of classes to output\n",
    "    'cord_syst'     : 'segy',   # Coordinate system used, default is 0,0. Set to 'segy' to give inputs in (inline,xline)\n",
    "    'save_pred'     : True,    # Save the prediction as a segy-cube\n",
    "    'save_location' : 'predictions\\\\BE_TEST_PREDS',       # file name for the saved prediction\n",
    "    'pred_batch'    : 1        # number of traces used to make batches of mini-cubes that are stored in memory at once\n",
    "    #'pred_batch' : train_dict['num_train_ex']//(pred_dict['section_edge'][5]-pred_dict['section_edge'][4])    #Suggested value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num epochs: 10\n",
      "num examples per epoch: 80000\n",
      "batch size: 32\n",
      "optimizer patience: 10\n",
      "Making class-adresses\n",
      "Finished making class-adresses\n",
      "Defining the buffer zone:\n",
      "(inl_min, inl_max, xl_min, xl_max, t_min, t_max)\n",
      "( 130 , 720 , 330 , 1220 , 124 , 1728 )\n",
      "( 30 , 620 , 30 , 920 , 30 , 431 )\n",
      "Epoch 1/10\n",
      "   4/1750 [..............................] - ETA: 364:34:07 - loss: 2.7335 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "output_dict1 = master(\n",
    "    segy_filename   = filenames,    # Seismic filenames\n",
    "    cube_incr       = cube_incr,    # Increments in each direction to create a training cube\n",
    "    train_dict      = train_dict,   # Input training dictionary\n",
    "    pred_dict       = pred_dict,    # Input prediction dictionary\n",
    "    mode            = 'train'     # Input mode ('train', 'predict', or 'full' for both training AND prediction)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
